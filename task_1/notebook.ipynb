{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd093f3-d652-47c8-be65-e30cf06a545b",
   "metadata": {},
   "source": [
    "## Import the libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a3b8a3-2dca-461e-b27d-45b307ebb103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1988243-9794-4d3f-b9b8-f969e82c7ec7",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42df7bdd-2eb4-45f8-aecb-2e89628405b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>dr. goldberg offers everything i look for in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Got a letter in the mail last week that said D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                               text\n",
       "0           0      4  dr. goldberg offers everything i look for in a...\n",
       "1           1      1  Unfortunately, the frustration of being Dr. Go...\n",
       "2           2      3  Been going to Dr. Goldberg for over 10 years. ...\n",
       "3           3      3  Got a letter in the mail last week that said D...\n",
       "4           4      0  I don't know what Dr. Goldberg was like before..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592249b-5d6c-4fdd-8c6f-364de32ea7c8",
   "metadata": {},
   "source": [
    "## Renaming the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b741d83-5bc5-462b-8ebd-fd97f4f5516b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>actual_stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dr. goldberg offers everything i look for in a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Got a letter in the mail last week that said D...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  actual_stars\n",
       "0  dr. goldberg offers everything i look for in a...             4\n",
       "1  Unfortunately, the frustration of being Dr. Go...             1\n",
       "2  Been going to Dr. Goldberg for over 10 years. ...             3\n",
       "3  Got a letter in the mail last week that said D...             3\n",
       "4  I don't know what Dr. Goldberg was like before...             0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={\n",
    "    \"text\": \"review_text\",\n",
    "    \"label\": \"actual_stars\"\n",
    "})\n",
    "\n",
    "df = df[[\"review_text\", \"actual_stars\"]]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb53ab-d59f-47e6-b9fb-9e0088823c60",
   "metadata": {},
   "source": [
    "## Call the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e34aea0a-3f9a-4247-b7b2-c849482130e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ollama(prompt, model=\"llama3.1:8b\"):\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        input=prompt,\n",
    "        text=True,\n",
    "        capture_output=True\n",
    "    )\n",
    "    return result.stdout.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1af4fae2-b1d8-4255-8e77-99334238741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_response(text):\n",
    "    try:\n",
    "        return json.loads(text), True\n",
    "    except json.JSONDecodeError:\n",
    "        return None, False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4ee21-ccf2-4f7d-ab3b-c0faab1ed89e",
   "metadata": {},
   "source": [
    "## Prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb474834-545e-48af-a66f-c0cba623a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_PROMPT = \"\"\"\n",
    "You are given a Yelp restaurant review.\n",
    "Predict the star rating from 1 to 5.\n",
    "\n",
    "Review:\n",
    "\"{review_text}\"\n",
    "\n",
    "Return your answer as JSON with:\n",
    "- predicted_stars\n",
    "- explanation\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc7bef6c-71f0-4031-b29c-5b4eda420a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline_prompt(review_text):\n",
    "    prompt = BASELINE_PROMPT.format(review_text=review_text)\n",
    "    return call_ollama(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d0182fa-cf37-4184-b63b-fd4e8fe2384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 200/200 [34:17<00:00, 10.29s/it]\n"
     ]
    }
   ],
   "source": [
    "baseline_results = []\n",
    "\n",
    "for _, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
    "    raw_output = run_baseline_prompt(row[\"review_text\"])\n",
    "    parsed_json, is_valid = parse_json_response(raw_output)\n",
    "\n",
    "    predicted_stars = None\n",
    "    explanation = None\n",
    "\n",
    "    if is_valid:\n",
    "        predicted_stars = parsed_json.get(\"predicted_stars\")\n",
    "        explanation = parsed_json.get(\"explanation\")\n",
    "\n",
    "    baseline_results.append({\n",
    "        \"review_text\": row[\"review_text\"],\n",
    "        \"actual_stars\": row[\"actual_stars\"],\n",
    "        \"predicted_stars\": predicted_stars,\n",
    "        \"json_valid\": is_valid,\n",
    "        \"explanation\": explanation\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "690537e3-3c2b-44e0-a000-848f6416a3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.01), np.float64(0.045))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "\n",
    "baseline_accuracy = (\n",
    "    baseline_df[\"predicted_stars\"] == baseline_df[\"actual_stars\"]\n",
    ").mean()\n",
    "\n",
    "baseline_json_validity = baseline_df[\"json_valid\"].mean()\n",
    "\n",
    "baseline_accuracy, baseline_json_validity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd18e75d-35e8-4e6a-8c1b-6811f9b61612",
   "metadata": {},
   "source": [
    "## Prompt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37dd749b-e691-46d5-890d-78fa3a8fedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V2 = \"\"\"\n",
    "You are a strict JSON-generating assistant.\n",
    "\n",
    "Your task is to read a Yelp restaurant review and predict the star rating from 1 to 5.\n",
    "\n",
    "Rules:\n",
    "- You MUST return valid JSON only.\n",
    "- Do NOT include any text outside the JSON.\n",
    "- \"predicted_stars\" must be an integer from 1 to 5.\n",
    "- \"explanation\" must be a short sentence explaining the rating.\n",
    "\n",
    "JSON format:\n",
    "{{\n",
    "  \"predicted_stars\": <int>,\n",
    "  \"explanation\": \"<string>\"\n",
    "}}\n",
    "\n",
    "Review:\n",
    "\"{review_text}\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc2c6179-5685-4d84-80f6-80f14dd7ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt_v2(review_text):\n",
    "    prompt = PROMPT_V2.format(review_text=review_text)\n",
    "    return call_ollama(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c227617-5d77-424f-8bea-c92b7f6a73ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 200/200 [10:56<00:00,  3.28s/it]\n"
     ]
    }
   ],
   "source": [
    "v2_results = []\n",
    "\n",
    "for _, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
    "    raw_output = run_prompt_v2(row[\"review_text\"])\n",
    "    parsed_json, is_valid = parse_json_response(raw_output)\n",
    "\n",
    "    predicted_stars = None\n",
    "    explanation = None\n",
    "\n",
    "    if is_valid:\n",
    "        predicted_stars = parsed_json.get(\"predicted_stars\")\n",
    "        explanation = parsed_json.get(\"explanation\")\n",
    "\n",
    "    v2_results.append({\n",
    "        \"review_text\": row[\"review_text\"],\n",
    "        \"actual_stars\": row[\"actual_stars\"],\n",
    "        \"predicted_stars\": predicted_stars,\n",
    "        \"json_valid\": is_valid,\n",
    "        \"explanation\": explanation\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71204e38-1b44-4480-b6bb-7346fb3b093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.14), np.float64(1.0))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2_df = pd.DataFrame(v2_results)\n",
    "\n",
    "v2_accuracy = (\n",
    "    v2_df[\"predicted_stars\"] == v2_df[\"actual_stars\"]\n",
    ").mean()\n",
    "\n",
    "v2_json_validity = v2_df[\"json_valid\"].mean()\n",
    "\n",
    "v2_accuracy, v2_json_validity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcad6f9-2f2c-400f-8b6d-33877feb92cd",
   "metadata": {},
   "source": [
    "## Prompt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abdee4af-1c26-40cb-ad12-5dbed1a94eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V3 = \"\"\"\n",
    "You are an expert sentiment analyst for restaurant reviews.\n",
    "\n",
    "Your task is to read a Yelp restaurant review and predict the star rating from 1 to 5.\n",
    "\n",
    "Rating guidelines:\n",
    "- 1 star: Very negative experience, strong complaints, not recommended.\n",
    "- 2 stars: Mostly negative, significant issues, disappointment.\n",
    "- 3 stars: Mixed or neutral experience, average or inconsistent.\n",
    "- 4 stars: Mostly positive, good experience, minor issues.\n",
    "- 5 stars: Very positive experience, highly satisfied, strongly recommended.\n",
    "\n",
    "You MUST follow these rules:\n",
    "- Return valid JSON only.\n",
    "- Do NOT include any text outside the JSON.\n",
    "- \"predicted_stars\" must be an integer from 1 to 5.\n",
    "- \"explanation\" must briefly justify the rating.\n",
    "\n",
    "Example 1:\n",
    "Review: \"The food was cold and the service was rude. I will not be coming back.\"\n",
    "Output:\n",
    "{{\n",
    "  \"predicted_stars\": 1,\n",
    "  \"explanation\": \"The review describes a very negative experience with poor food quality and service.\"\n",
    "}}\n",
    "\n",
    "Example 2:\n",
    "Review: \"The food tasted great and the staff were friendly, but the wait time was a bit long.\"\n",
    "Output:\n",
    "{{\n",
    "  \"predicted_stars\": 4,\n",
    "  \"explanation\": \"The experience was mostly positive despite a minor issue with wait time.\"\n",
    "}}\n",
    "\n",
    "Now analyze the following review.\n",
    "\n",
    "Review:\n",
    "\"{review_text}\"\n",
    "\n",
    "Output JSON:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5385a80-a685-4232-a5de-faca07b8a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt_v3(review_text):\n",
    "    prompt = PROMPT_V3.format(review_text=review_text)\n",
    "    return call_ollama(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78d3f1f0-d92c-41cb-a944-ed4d63a67542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"predicted_stars\": 2,\n",
      "  \"explanation\": \"The reviewer had mixed feelings about the experience, mentioning decent taste but poor variety and service.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(run_prompt_v3(df_sample.loc[0, \"review_text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2a47368-fec4-4796-8305-4f9503277873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 200/200 [14:18<00:00,  4.29s/it]\n"
     ]
    }
   ],
   "source": [
    "v3_results = []\n",
    "\n",
    "for _, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
    "    raw_output = run_prompt_v3(row[\"review_text\"])\n",
    "    parsed_json, is_valid = parse_json_response(raw_output)\n",
    "\n",
    "    predicted_stars = None\n",
    "    explanation = None\n",
    "\n",
    "    if is_valid:\n",
    "        predicted_stars = parsed_json.get(\"predicted_stars\")\n",
    "        explanation = parsed_json.get(\"explanation\")\n",
    "\n",
    "    v3_results.append({\n",
    "        \"review_text\": row[\"review_text\"],\n",
    "        \"actual_stars\": row[\"actual_stars\"],\n",
    "        \"predicted_stars\": predicted_stars,\n",
    "        \"json_valid\": is_valid,\n",
    "        \"explanation\": explanation\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31991291-2239-47e4-b5c3-24e149d47f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.135), np.float64(1.0))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v3_df = pd.DataFrame(v3_results)\n",
    "\n",
    "v3_accuracy = (\n",
    "    v3_df[\"predicted_stars\"] == v3_df[\"actual_stars\"]\n",
    ").mean()\n",
    "\n",
    "v3_json_validity = v3_df[\"json_valid\"].mean()\n",
    "\n",
    "v3_accuracy, v3_json_validity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901eac72-fe97-4758-8c65-a460c62ce5e0",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cbf478-856b-4b50-8eb9-334fef3b53bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f16ea7f1-2e54-4f2c-a8b1-2e667575d632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_09d2c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_09d2c_level0_col0\" class=\"col_heading level0 col0\" >Prompt Version</th>\n",
       "      <th id=\"T_09d2c_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_09d2c_level0_col2\" class=\"col_heading level0 col2\" >JSON Validity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_09d2c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_09d2c_row0_col0\" class=\"data row0 col0\" >Baseline (V1)</td>\n",
       "      <td id=\"T_09d2c_row0_col1\" class=\"data row0 col1\" >1.00%</td>\n",
       "      <td id=\"T_09d2c_row0_col2\" class=\"data row0 col2\" >4.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_09d2c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_09d2c_row1_col0\" class=\"data row1 col0\" >Strict JSON (V2)</td>\n",
       "      <td id=\"T_09d2c_row1_col1\" class=\"data row1 col1\" >14.00%</td>\n",
       "      <td id=\"T_09d2c_row1_col2\" class=\"data row1 col2\" >100.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_09d2c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_09d2c_row2_col0\" class=\"data row2 col0\" >Examples + Guidelines (V3)</td>\n",
       "      <td id=\"T_09d2c_row2_col1\" class=\"data row2 col1\" >13.50%</td>\n",
       "      <td id=\"T_09d2c_row2_col2\" class=\"data row2 col2\" >100.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x110da5090>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    \"Prompt Version\": [\"Baseline (V1)\", \"Strict JSON (V2)\", \"Examples + Guidelines (V3)\"],\n",
    "    \"Accuracy\": [baseline_accuracy, v2_accuracy, v3_accuracy],\n",
    "    \"JSON Validity\": [baseline_json_validity, v2_json_validity, v3_json_validity]\n",
    "})\n",
    "\n",
    "comparison_df.style.format({\n",
    "    \"Accuracy\": \"{:.2%}\",\n",
    "    \"JSON Validity\": \"{:.2%}\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2da2435-33b4-414d-9342-1025d905c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_star_distance(df):\n",
    "    valid_df = df.dropna(subset=[\"predicted_stars\"])\n",
    "    return np.mean(\n",
    "        np.abs(valid_df[\"predicted_stars\"] - valid_df[\"actual_stars\"])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9337f011-505f-4ce9-badf-dcf141c81fae",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "1. Baseline Prompt (V1)\n",
    "The baseline prompt was intentionally minimal, resulting in extremely low accuracy (1%) and poor JSON validity (4%). This demonstrated that unconstrained prompts are unreliable for structured prediction tasks.\n",
    "2. Prompt Version 2 (Strict JSON)\n",
    "Introducing explicit role instructions and a strict JSON schema led to a dramatic improvement in JSON validity (100%) and a substantial increase in accuracy (~14%). This shows that output constraints are critical when using LLMs for structured prediction.\n",
    "3. Prompt Version 3 (Examples + Guidelines)\n",
    "Adding rating guidelines and few-shot examples maintained perfect JSON validity but did not significantly improve accuracy. This suggests that while examples improve consistency and explanation quality, they may also introduce bias depending on example selection. This highlights a trade-off between calibration and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba1839d-6bf4-401e-beda-2ff7ff948fff",
   "metadata": {},
   "source": [
    "## Extra: calculated mean square star distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca53506e-4e6f-41d3-a5ca-428be0d235a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(1.2222222222222223), np.float64(1.05), np.float64(1.11))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_distance = mean_absolute_star_distance(baseline_df)\n",
    "v2_distance = mean_absolute_star_distance(v2_df)\n",
    "v3_distance = mean_absolute_star_distance(v3_df)\n",
    "\n",
    "baseline_distance, v2_distance, v3_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66722c7a-a60a-4031-97e3-5541a0f4f330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_bdbf0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bdbf0_level0_col0\" class=\"col_heading level0 col0\" >Prompt Version</th>\n",
       "      <th id=\"T_bdbf0_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_bdbf0_level0_col2\" class=\"col_heading level0 col2\" >JSON Validity</th>\n",
       "      <th id=\"T_bdbf0_level0_col3\" class=\"col_heading level0 col3\" >Mean Absolute Star Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bdbf0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_bdbf0_row0_col0\" class=\"data row0 col0\" >Baseline (V1)</td>\n",
       "      <td id=\"T_bdbf0_row0_col1\" class=\"data row0 col1\" >1.00%</td>\n",
       "      <td id=\"T_bdbf0_row0_col2\" class=\"data row0 col2\" >4.50%</td>\n",
       "      <td id=\"T_bdbf0_row0_col3\" class=\"data row0 col3\" >1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdbf0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_bdbf0_row1_col0\" class=\"data row1 col0\" >Strict JSON (V2)</td>\n",
       "      <td id=\"T_bdbf0_row1_col1\" class=\"data row1 col1\" >14.00%</td>\n",
       "      <td id=\"T_bdbf0_row1_col2\" class=\"data row1 col2\" >100.00%</td>\n",
       "      <td id=\"T_bdbf0_row1_col3\" class=\"data row1 col3\" >1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdbf0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_bdbf0_row2_col0\" class=\"data row2 col0\" >Examples + Guidelines (V3)</td>\n",
       "      <td id=\"T_bdbf0_row2_col1\" class=\"data row2 col1\" >13.50%</td>\n",
       "      <td id=\"T_bdbf0_row2_col2\" class=\"data row2 col2\" >100.00%</td>\n",
       "      <td id=\"T_bdbf0_row2_col3\" class=\"data row2 col3\" >1.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x110d14050>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    \"Prompt Version\": [\n",
    "        \"Baseline (V1)\",\n",
    "        \"Strict JSON (V2)\",\n",
    "        \"Examples + Guidelines (V3)\"\n",
    "    ],\n",
    "    \"Accuracy\": [\n",
    "        baseline_accuracy,\n",
    "        v2_accuracy,\n",
    "        v3_accuracy\n",
    "    ],\n",
    "    \"JSON Validity\": [\n",
    "        baseline_json_validity,\n",
    "        v2_json_validity,\n",
    "        v3_json_validity\n",
    "    ],\n",
    "    \"Mean Absolute Star Distance\": [\n",
    "        baseline_distance,\n",
    "        v2_distance,\n",
    "        v3_distance\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison_df.style.format({\n",
    "    \"Accuracy\": \"{:.2%}\",\n",
    "    \"JSON Validity\": \"{:.2%}\",\n",
    "    \"Mean Absolute Star Distance\": \"{:.2f}\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024d56b-4b02-4482-ac0b-1891bbfba1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7b66b89-cc74-42d7-8021-33edc6607433",
   "metadata": {},
   "source": [
    "| Prompt                         | What the numbers *mean*                                                                                                                                                 |\n",
    "| ------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Baseline (V1)**              | Very low accuracy and JSON validity → confirms that unconstrained prompts are unreliable. Mean distance 1.22 indicates near-random guessing.                            |\n",
    "| **Strict JSON (V2)**           | Massive jump in JSON validity (4.5% → 100%) and accuracy (1% → 14%). Mean distance drops to 1.05, showing improved calibration.                                         |\n",
    "| **Examples + Guidelines (V3)** | Accuracy stays similar, JSON validity remains perfect, but mean distance slightly worsens (1.11). This suggests examples improved consistency but introduced mild bias. |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fnyd",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
